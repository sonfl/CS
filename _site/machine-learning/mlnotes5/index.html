<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
       &middot; Computer Science and Math notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/CS/public/css/poole.css">
  <link rel="stylesheet" href="/CS/public/css/syntax.css">
  <link rel="stylesheet" href="/CS/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/CS/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/CS/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  
<script>
function cvonk_ResizeMathJax() {
    jQuery('.MathJax_Display').each(function(ii, obj) {
        var latex = obj.children[0];
        var w = latex.offsetWidth;
        var h = latex.offsetHeight;    
        var W = obj.offsetWidth;
        if (w > W) {
            obj.style.fontSize = 85 * W / w + "%";
        }
    });
}
window.MathJax = {
    AuthorInit: function() {
    MathJax.Hub.Register.StartupHook("Begin", function() {
        MathJax.Hub.Queue(function() {
        cvonk_ResizeMathJax();
        });
    });
    },
    jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
    extensions: ["tex2jax.js"]
};
window.addEventListener("resize", function() {
    cvonk_ResizeMathJax();  
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    },
    "HTML-CSS": {
        styles: {".MathJax": {
          color: "#033 ! important"}
                  }
    },
    SVG: { 
      linebreaks: { automatic: true } 
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>

</script>


  <body class="theme-base-0f layout-reverse">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Notes on CS made by an independent learner. Want to discuss something? Here we go: <a href="https://t.me/slonecznik">Telegram</a>, <a href="https://github.com/sonfl">Github</a></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/CS/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/CS/machine-learning/">Andrew Ng's Machine Learning</a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    <!--
    <a class="sidebar-nav-item" href="/archive/v.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
  </nav>
   -->

</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/CS/" title="Home">Computer Science and Math notes</a>
            <small>powered by coffee and whiteboard</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <p>Generative Learning is an approach to decide whether $x$ is in set 1 or set 2 of data. We do it by means of making the model to predict set 1 and another model to predict set 2. And assign $x$ to the set, where the probability is greater.</p>

<p>Therefore discriminative are of form: $p(y \vert x)$</p>

<p>Generative: $p(x \vert y)$ and $p(y)$</p>

<p>After we model our models(above), we can use Bayes rule to get:</p>

<script type="math/tex; mode=display">p(y \vert x) = \frac{p(x \vert y)p(y)}{p(x)}</script>

<p>where $p(x) = p(x \vert y=1)p(y=1)+p(x \vert y=0)p(y=0)$</p>

<p>The first example of such a learning is</p>

<blockquote>
  <p>Gaussian discriminant analysis(GDA)</p>
</blockquote>

<p>In this model we assume that $p(x \vert y) is distributed according to multivariate normal distribution, which is:</p>

<p><script type="math/tex">p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}</script>,</p>

<p>where $\Sigma$ is a covariance matrix</p>

<p>Let’s have an example of GDA:</p>

<p>Our model will be as such:</p>

<p>$ y \sim$ Bernoulli($\phi$)</p>

<p>$x \vert y=0 \sim \mathcal{N}(\mu_{0}, \Sigma)$</p>

<p>$x \vert y=1 \sim \mathcal{N}(\mu_{1}, \Sigma)$</p>

<p>Therefore we have:</p>

<p>$p(y) = \phi^{y}(1-\phi)^{1-y}$</p>

<p>$p(x \vert y=0) = \frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_{0})^{T}\Sigma^{-1}(x-\mu_{0})}$</p>

<p>$p(x \vert y=1) = \frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_{1})^{T}\Sigma^{-1}(x-\mu_{1})}$</p>

<p>And now our dear log likelihood:</p>

<p>$l(\phi, \mu_{0}, \mu_{1}, \Sigma) = log \prod_{i=1}^{m}p(x^{(i)}, y^{(i)}) = log \prod_{i=1}^{m}p(x^{(i)} \vert y^{(i)})p(y^{(i)})$</p>

<p>To predict we need $argmax_{y}P(y \vert x) = argmax_{y}P(x \vert y)P(y)$</p>

<p>Also by some magic steps we can observe that somehow $p(y=1 \vert x; \Sigma, \mu_{0}, \mu_{1}) = \frac{1}{1+e^{-\theta^{T}x}}$, which we used to model $p(y=1 \vert x)$</p>

<p>So when we would prefer GDA over logistic regression and vica versa?</p>

<p>Apparently, if we have multivariate gaussian we get logistical regression, but the converse is not true. So we may think of GDA as a stronger modelling. Therefore it is more efficient. But on the other hand, if we make incorrect assumptions, we would be better off with a weaker logistic regression, which would handle all inconsistencies with the data.</p>

<blockquote>
  <p>Naive Bayes</p>
</blockquote>

<p>Spam filter is a good example of Naive Bayes. Suppose we have a vector $x = \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}$ where 1 is a presence of some particular word and 0 is its abscence. Then this vector represents a particular mail which we’ll try to classify.</p>

<p>Now we need to model $P(x \vert y)$. Now we will assume that $x_{i}$ is conditionally independent give y (which is rather not true, since if we get a mail where we have a word “Java”, it’s more probable to have other CS-related word), but it still works. This assumption has its name: Naive Bayes assumption.
$P(x_{1000} \vert y) = P(x_{1000} \vert y, x_{1200})$</p>

<p>(Note it’s not a full independence of events)
Suppose our dictionary (vector x) has dimension of 50000.</p>

<p><script type="math/tex">P(x_{1},...,x_{50000} \vert y)
= P(x_{1} \vert y) P(x_{2} \vert y, x_{1}) ... P(x_{50000} \vert y, x_{1}, x_{2},...,x_{49999})</script>
<script type="math/tex">= P(x_{1} \vert y)P(x_{2} \vert y)...P(x_{50000} \vert y) = \prod_{j=1}^{n}P(x_{j} \vert y)</script></p>

<p>Our model is parameterized by $\phi_{j \vert y=1} = P(x_{j} = 1 \vert y=1)$,$\phi_{j \vert y=0} = P(x_{j} = 1 \vert y=0)$ and $\phi_{y} = P(y=1)$</p>

<p>Then for our training set {$(x{(i)},y^{(i)}); i = 1,…,m$} we can write likelihood:</p>

<script type="math/tex; mode=display">L(\phi_{y=1}, \phi_{j \vert y=1}, \phi_{j \vert y=0}) = \prod_{i=1}^{m}P(x{(i)},y^{(i)})</script>

<p>After maximazing that likelihood we get:</p>

<script type="math/tex; mode=display">\phi_{j \vert y=1} = \frac{\sum_{i=1}^{m}1\{x_{j}^{(i)} = 1 \wedge y^{(i)} = 1\}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}}</script>

<script type="math/tex; mode=display">\phi_{j \vert y=0} = \frac{\sum_{i=1}^{m}1\{x_{j}^{(i)} = 1 \wedge y^{(i)} = 0\}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}}</script>

<script type="math/tex; mode=display">\phi_{y} = \frac{\sum_{i=1}^{m}1\{y^{(i)} = 1\}}{m}</script>

<p>So we can easily compute such a thing:</p>

<p><script type="math/tex">P(y=1 \vert x) = \frac{P(x \vert y=1) P(y=1)}{P(x)}</script> 
<script type="math/tex">= \frac{(\prod_{j=1}^{n}P(x_{j} \vert y=1)) P(y=1)}{(\prod_{j=1}^{n}P(x_{j} \vert y=1))P(y=1) + (\prod_{j=1}^{n}P(x_{j} \vert y=0))P(y=0)}</script></p>

<p>And then we pick whichever has the higher probability.</p>

<blockquote>
  <p>Laplace smoothing</p>
</blockquote>

<p>If we get a new word we’ve never seen before,by our algorithm above, we automatically set $\phi_{y=1} = 0$ and $\phi_{y=0} = 0$ so $P(y=1 \vert x) = \frac{0}{0}$ which is undefined.</p>

<p>To combat an issue we may change a little bit a definition of $\phi_{j}$ to be:</p>

<script type="math/tex; mode=display">\phi_{j} = \frac{\sum_{i=1}^{m}1\{z^{(i)} = j\} + 1}{m + k}</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
