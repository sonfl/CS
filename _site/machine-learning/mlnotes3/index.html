<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
       &middot; Computer Science and Math notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/CS/public/css/poole.css">
  <link rel="stylesheet" href="/CS/public/css/syntax.css">
  <link rel="stylesheet" href="/CS/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/CS/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/CS/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  
<script>
function cvonk_ResizeMathJax() {
    jQuery('.MathJax_Display').each(function(ii, obj) {
        var latex = obj.children[0];
        var w = latex.offsetWidth;
        var h = latex.offsetHeight;    
        var W = obj.offsetWidth;
        if (w > W) {
            obj.style.fontSize = 85 * W / w + "%";
        }
    });
}
window.MathJax = {
    AuthorInit: function() {
    MathJax.Hub.Register.StartupHook("Begin", function() {
        MathJax.Hub.Queue(function() {
        cvonk_ResizeMathJax();
        });
    });
    },
    jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
    extensions: ["tex2jax.js"]
};
window.addEventListener("resize", function() {
    cvonk_ResizeMathJax();  
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    },
    "HTML-CSS": {
        styles: {".MathJax": {
          color: "#033 ! important"}
                  }
    },
    SVG: { 
      linebreaks: { automatic: true } 
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>

</script>


  <body class="theme-base-0f layout-reverse">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Notes on CS made by an independent learner. Want to discuss something? Here we go: <a href="https://t.me/slonecznik">Telegram</a>, <a href="https://github.com/sonfl">Github</a></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/CS/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/CS/machine-learning/">Andrew Ng's Machine Learning</a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    <!--
    <a class="sidebar-nav-item" href="/archive/v.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
  </nav>
   -->

</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/CS/" title="Home">Computer Science and Math notes</a>
            <small>powered by coffee and whiteboard</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <blockquote>
  <p>Some of the missed steps in calculations might be in the Questions section. Or might not…</p>
</blockquote>

<p>“Parametric” learning algorithm
    - $\theta$’s fixed set of parameters
“Non-parametric” learning algorithm
    - # of params growns with m</p>

<p>As an example of non-parametric algorithm we have Locally weighted regression defined in such a form:</p>

<p><script type="math/tex">\sum_{i}w^{(i)}(y^{(i)}-\Theta^{T}x^{(i)})^{2}</script>, where $w^{(i)} = e^{-\frac{(x^{(i)}-x)^{2}}{2}}$</p>

<p>$w^{(i)}$ has nothing to do with Gaussian’s, just in case.</p>

<p>Probabilistic interpretation:</p>

<p>This interpretation is full of assumption, which will be introduced along the way, so get ready:</p>

<p>Assume $y^{(i)} = \Theta^{T}x^{(i)}+\epsilon^{(i)}$, where $\epsilon^{(i)}$ is an error term</p>

<p>$\epsilon^{(i)} \sim \mathcal{N}(0,\sigma^{2})$</p>

<p>$P(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\epsilon^{(i)})^{2}}{2\sigma^{2}}}$</p>

<p>$P(y^{(i)} \vert x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}}}$, where</p>

<p>$(y^{(i)} \vert x^{(i)};\theta) \sim \mathcal{N}(\theta^{T}x^{(i)}, \sigma^{2})$</p>

<p>So, let’s review these lines: as Andrew notices errors are usually normally distributed. Let’s remember central limit theorem that says that when independent variables are added its sum tends to normal distribution. Here is our next assumption: $\epsilon^{(i)}$ are independently identical distributions.</p>

<p>So bearing that in mind, we can deduce that every outcome (y in our example) has some noise, with the same variance as the error, since error is the reason of that normal distribution.</p>

<p>So the likelihood function would look like this:</p>

<p>$L(\theta) = P(\vec{y} \vert X;\Theta)
= \prod_{i=1}^{m}P(y^{(i)} \vert x^{(i)};\theta)
=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}}}$</p>

<p>To be more clear, we mean likelihood of parameters and probability of data.</p>

<p>Maximum likelihood: choose $\theta$ to maximize $L(\theta)$</p>

<p>To make our lives easier let’s introduce</p>

<p>$l(\theta) = logL(\theta) = 
log\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{(…)}$</p>

<p>$= \sum_{i=1}^{m}log(\frac{1}{\sqrt{2\pi}\sigma}e^{…}) = mlog\frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^{m}-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}}$</p>

<p>So maximazing $l(\theta)$ is the same as minimazing $\frac{\sum_{i=1}^{m}(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2} = J(\theta)$</p>

<p>As we can see here $\sigma^{2}$ doesn’t really matter. Whatever the sigma is, we’ll end up with $J(\theta)$. We will comeback to it later.</p>

<p>Welcome to the logistic regression, buddy!
Basically it’s not quite linear regression since $y \in \{0,1\}$. Also it’s usually a bad idea to apply logistic regression to linear one.</p>

<p>Sooooooo, let’s define the function for this type of regression!</p>

<p>$h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}$, where $g(z)$ is a sigmoid function</p>

<p><img src="/CS/public/Plot-of-the-sigmoid-function.png" /></p>

<p>The reasons behind this very function will be later in this course.</p>

<p>$P(y=1 \vert x; \theta) = h_{\theta}(x)$</p>

<p>$P(y= \vert x; \theta) = 1 - h_{\theta}(x)$</p>

<p>$P(y \vert x; \theta) = h_{\theta}(x)^{y}(1 - h_{\theta}(x))^{1-y}$</p>

<p>Isn’t it a neat way to write these eqs??</p>

<p>$L(\theta) = P(\vec{y} \vert X;\Theta) = \prod_{i}P(y^{(i)} \vert x^{(i)}; \theta)$</p>

<p>$=\prod_{i}h_{\theta}(x^{(i)})^{y^{(i)}}(1 - h_{\theta}(x^{(i)}))^{1-y^{(i)}}$</p>

<p>Hello good old $l(\theta)$</p>

<p>$l(\theta) = logL(\theta) = \sum_{i}(y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})))$</p>

<p>So we can apply our already known gradient ascend to compute $\theta$:</p>

<p>$\theta = \theta + \alpha\nabla_{\theta}l(\theta)$</p>

<p>(please notice that we put a plus sign, because we need to find maximum, please refer to the Questions section of 2nd lecture)</p>

<p>By some woo-doo magic we get:</p>

<p>$\frac{\partial}{\partial \theta_{j}}l(\theta) = \sum_{i=1}^{m}(y^{(i)} - h_{\theta}(x^{(i)})x_{j}^{(i)}$</p>

<p>Which is exacly the same thing as in linear regression. Except $h_{\theta}(x^{(i)})$. Why it is so - see in the next lecture!</p>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
