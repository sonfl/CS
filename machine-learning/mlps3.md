---
layout: default
mathjax: true
---
[The task can be found here.](/CS/public/ps1.pdf)

Problem 1: Logistic regression
    Part (a):

$J(\theta) = \frac{1}{m}\sum_{k=1}^{m}log(1+e^{ -y^{(k)} \theta^{T} x^{(k)}} ) = -\frac{1}{m}\sum_{k=1}^{m}log(h(y^{(k)}x^{(k)}))$

$\frac{\partial}{\partial z}g(z) = \frac{\partial}{\partial z} \frac{1}{(1+e^{-z})} = -\frac{1}{(1+e^{-z})^{2}}(-e^{-z}) $ $
= \frac{e^{-z}+1-1}{(1+e^{-z})^{2}} = g(z) - \frac{1}{(1+e^{-z})^{2}} = g(z)(1-g(z))$

Hessian $H_{ij} = \frac{\partial}{\partial \theta_{j}} (\frac{\partial}{\partial \theta_{i}} J)$

$\frac{\partial J}{\partial \theta_{i}} = -\frac{1}{m}\sum_{k=1}^{m} \frac{\partial}{\partial \theta_{i}}log(g(z)) = -\frac{1}{m}\sum_{k=1}^{m} \frac{1}{g(z)}g'(z)\frac{\partial z}{\partial \theta_{i}}$
$= -\frac{1}{m}\sum_{k=1}^{m} (1-g(z))x_{i}^{(k)}y^{(k)}$

$H = \frac{\partial}{\partial \theta_{j}} (\frac{\partial}{\partial \theta_{i}} J) = -\frac{1}{m}\sum_{k=1}^{m} \frac{\partial}{\partial \theta_{j}} (1-g(z))x_{i}^{(k)}y^{(k)} = \frac{1}{m}\sum_{k=1}^{m}g(z)(1-g(z))x_{i}^{(k)}x_{j}^{(k)}(y^{(k)})^{2}$

I wasted 45 minutes of my life trying to understand why people on the web erase the $(y^{(k)})^{2}$.... Check the task, it's either -1 or 1.

Remember the Hessian equation: $z^{T}Hz = z^{T}\frac{1}{m}\sum_{k=1}^{m}g(z)(1-g(z))x_{i}^{(k)}x_{j}^{(k)}z $
$= \frac{1}{m}\sum_{k=1}^{m}g(z)(1-g(z))z^{T}x_{i}^{(k)}x_{j}^{(k)}z = \frac{1}{m}\sum_{k=1}^{m}g(z)(1-g(z))(x^{T}z)^{2}$

Since $0< g(z)< 1$, $1-g(z)>0$ and $(x^{T}z)^{2} > 0$ we claim that H is positive semidefinite matrix, therefore $J(\theta)$ is convex, has no local minimums, only global.



